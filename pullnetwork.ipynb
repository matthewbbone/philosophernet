{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 32-bit",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "27e51d9869a86ed31c6d232a30492375277492976997f955be30b722c624d282"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import pickle\n",
    "import csv\n",
    "import re\n",
    "import datetime\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"https://en.wikipedia.org\"\n",
    "descartes = \"/wiki/Ren%C3%A9_Descartes\"\n",
    "#print(parse_connections(prefix, descartes))\n",
    "ibn = \"/wiki/Ibn_al-Haytham\"\n",
    "#print(parse_connections(prefix, ibn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is passed two components of a url, the general English wikipedia url\n",
    "# and the reference to a particular page. It then pulls the html of the page specified\n",
    "# and turns it into a parseable \"soup\" using the BeautifulSoup package. It verifies\n",
    "# that the page has \"div\" components with the labels \"Influenced\" and \"Influences\" or \n",
    "# \"Influenced by\". These are reliable indicators of the page being a philosopher or\n",
    "# at the very least an relevant figure in philosophy. If the page doesn't match an expected\n",
    "# format then it returns four None. Otherwise, it calls influences_parse or influenced_by_parse\n",
    "# depending on the pages format.\n",
    "def parse_connections(prefix, ref):\n",
    "    page = requests.get(prefix + ref)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "    influences = soup.find(\"div\", string = \"Influences\")\n",
    "    influenced = soup.find(\"div\", string = \"Influenced\")\n",
    "    influenced_by = soup.find(\"div\", string = \"Influenced\\xa0by\")\n",
    "\n",
    "    if influences is None and influenced_by is None: return None, None, None, None\n",
    "    if influenced is None: return None, None, None, None\n",
    "\n",
    "    if influenced_by is None: return influences_parse(soup, ref)\n",
    "    else: return influenced_by_parse(soup, ref)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# It's important to know that most philosophers had both \"Influenced\" and \"Influences\" as\n",
    "# \"div\" components whereas most Islamic scholars had \"Influenced\" and \"Influenced by\". The\n",
    "# division of the parse functions is to deal with these two cases.\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# This function parses pages that have a \"div\" component called \"Influences\". It\n",
    "# has a page's html \"soup\" passed as well as it's ref. It begins by finding the title\n",
    "# text of the page which is the philosopher's name. It then finds the \"divs\" of class\n",
    "# \"center\". The first two of which are (hopefully) reliably the \"Influences\" and\n",
    "# \"Influenced\" components. Then the groups of philosophers labeled \"Influences\" and \n",
    "# \"Influenced\" are converted into list objects. \n",
    "def influences_parse(soup, ref):\n",
    "\n",
    "    name = soup.find(\"h1\", id = \"firstHeading\").text\n",
    "    divs = soup.find_all(\"div\", class_ = \"center\")\n",
    "\n",
    "    infs = divs[0]\n",
    "    infd = divs[1]\n",
    "\n",
    "    infs = bs4_list_convert(infs)\n",
    "    infd = bs4_list_convert(infd)\n",
    "\n",
    "    return name, ref, infs, infd\n",
    "\n",
    "# This function does the same as above except instead of finding \"div\" components \n",
    "# of class \"center\" it must find \"ul\" components of class \"NavContent\". This is due\n",
    "# to the different html structure of pages with \"div\" components called \"Influenced by\". \n",
    "def influenced_by_parse(soup, ref):\n",
    "\n",
    "    name = soup.find(\"h1\", id = \"firstHeading\").text\n",
    "    uls = soup.find_all(\"ul\", class_ = \"NavContent\")\n",
    "\n",
    "    infs = uls[0]\n",
    "    infd = uls[1]\n",
    "\n",
    "    infs = bs4_list_convert(infs)\n",
    "    infd = bs4_list_convert(infd)\n",
    "\n",
    "    return name, ref, infs, infd\n",
    "\n",
    "# This function parses an html component containing a list of philosophers. \n",
    "# It receives a component, div, and finds all of the \"a\" components and extracts\n",
    "# their text and ref (this may cause some issues with philosophers whose name is \n",
    "# different in their page heading than when their in a list). It then returns a list\n",
    "# of all the names and refs of all of the philosophers. \n",
    "def bs4_list_convert(div):\n",
    "    l = []\n",
    "\n",
    "    for person in div.find_all(\"a\", href = True):\n",
    "        name = person.get_text()\n",
    "        ref = person[\"href\"]\n",
    "\n",
    "        if not \"[\" in name and not \"wikipedia\" in ref:\n",
    "            l.append([name,ref]) \n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function is passed two string components of a url for a philosopher's wiki\n",
    "# page as well as a dictionary of philosophers. If the philosophers dictionary\n",
    "# is None then the function simply parses the wiki page of the one philosopher and\n",
    "# returns a dictionary with just that entry. If the dictionary already has entries\n",
    "# then it iterates through that philosophers influencers and influencees, parses\n",
    "# their wiki page and adds them to the philosophers dictionary. It then returns the\n",
    "# larger dictionary. \n",
    "def phil_crawl(prefix, ref, philosophers):\n",
    "\n",
    "    name, ref, infs, infd = parse_connections(prefix, ref)\n",
    "    if philosophers is None: return {name:[ref, [row[0] for row in infs], [row[0] for row in infd]]}\n",
    "\n",
    "    for person in infs:\n",
    "        if not person[0] in philosophers.keys():\n",
    "            name, ref, ifs, ifd = parse_connections(prefix, person[1])\n",
    "            if not name is None: \n",
    "                philosophers[name] = [ref, [row[0] for row in ifs], [row[0] for row in ifd]]\n",
    "\n",
    "    for person in infd: \n",
    "        if not person[0] in philosophers.keys():\n",
    "            name, ref, ifs, ifd = parse_connections(prefix, person[1])\n",
    "            if not name is None: \n",
    "                philosophers[name] = [ref, [row[0] for row in ifs], [row[0] for row in ifd]]\n",
    "\n",
    "    return philosophers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function is where the network is fully collected. It is passed two\n",
    "# components of a philosopher's wiki page url to start the web crawl at. \n",
    "# It also takes the number of iterations which is essentially the number of\n",
    "# steps away from the starting philosopher you would want to go. It starts\n",
    "# by initializing the dictionary with a phil_crawl that returns a dictionary\n",
    "# with just one entry. Then it iteratively goes through the entire dictionary\n",
    "# and for any entry that it hasn't already, it uses phil_crawl to add all\n",
    "# their connections to the dictionary. This results in a network with all the\n",
    "# connections to the first philosopher that lie within the same number of\n",
    "# degree as iterations\n",
    "def iterated_crawl(prefix, ref, iterations):\n",
    "    phils = phil_crawl(prefix, ref, None)\n",
    "    temp = phils.copy()\n",
    "    searched = []\n",
    "    print(len(phils))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        for key, value in phils.items():\n",
    "            if not key in searched: \n",
    "                temp = phil_crawl(prefix, value[0], temp).copy()\n",
    "                searched.append(key)\n",
    "        phils = temp.copy()\n",
    "        print(len(phils))\n",
    "\n",
    "    return phils\n",
    "    \n",
    "# philosophers = iterated_crawl(prefix, descartes, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saves the network in .pkl file\n",
    "# f = open(r'philosophers4.pkl', 'wb')\n",
    "# pickle.dump(phil_4, f)\n",
    "# f.close()\n",
    "\n",
    "# loads the first version of the network from .pkl file \n",
    "# (didn't account for \"influenced by\" formats)\n",
    "f = open(r'philosophers.pkl', 'rb')\n",
    "phil_1 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# loads the second version of the network from .pkl file \n",
    "# (now accounts for \"influenced by\")\n",
    "f = open(r'philosophers2.pkl', 'rb')\n",
    "phil_2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# loads the third version of the network from .pkl file \n",
    "# (more efficient crawling for some reason has more nodes)\n",
    "f = open(r'philosophers3.pkl', 'rb')\n",
    "phil_3 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# loads the fourth version of the network from .pkl file \n",
    "# (includes date of birth and death and nationality)\n",
    "f = open(r'philosophers4.pkl', 'rb')\n",
    "phil_4 = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks for which entries were in network version 2\n",
    "# that weren't in network version 1\n",
    "for p in phil_2.keys():\n",
    "#    if not p in phil_1.keys():\n",
    "#        print(p)\n",
    "\n",
    "#print(\"------------------\")\n",
    "\n",
    "# checks for which entries were in network version 3\n",
    "# that weren't in network version 2\n",
    "#for p in phil_3.keys():\n",
    "#    if not p in phil_2.keys():\n",
    "#        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function uses the dateutil.parser to interpret a wide\n",
    "# array of date formats and convert it into datetime objects.abs\n",
    "# Since it will only interpret years with four digits I added some\n",
    "# logic to add 0 to the front of years with 1, 2 or 3 digits\n",
    "def date_convert(date): \n",
    "\n",
    "    if date is None: return None \n",
    "    elif date.isdigit() and len(date) == 3: \n",
    "        return datetime.datetime.strptime(\"0\"+date, '%Y').date()\n",
    "    elif date.isdigit() and len(date) == 2:\n",
    "        return datetime.datetime.strptime(\"00\"+date, '%Y').date()\n",
    "    elif date.isdigit() and len(date) == 1:\n",
    "        return datetime.datetime.strptime(\"000\"+date, '%Y').date()\n",
    "    elif re.search(\"[0-9]{3}-0*-0*\", date): \n",
    "        return datetime.datetime.strptime(\"0\"+date[0:3], '%Y').date()\n",
    "    elif re.search(\"[0-9]{2}-0*-0*\", date): \n",
    "        return datetime.datetime.strptime(\"00\"+date[0:2], '%Y').date()\n",
    "    elif re.search(\"[0-9]{1}-0*-0*\", date): \n",
    "        return datetime.datetime.strptime(\"000\"+date[0:1], '%Y').date()  \n",
    "    else: return dateutil.parser.parse(date)\n",
    "\n",
    "\n",
    "#for i in range(len(phil_4)):\n",
    "#    p = list(phil_4.values())[i]\n",
    "#    print(\"index: \", i, \"\\t\",p[3],\"\\t\", p[4],\"\\n\")\n",
    "#    print(date_convert(date_clean(p[3])),\"\\n\")\n",
    "#    print(date_convert(date_clean(p[4])),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This function takes in a messy string that contains a date\n",
    "# and pulls that date out and returns it. \n",
    "def date_clean(date):\n",
    "\n",
    "    if date is None: return None\n",
    "\n",
    "    # \"^[0|00] Month [00|000|0000]\"\n",
    "    temp = re.search(\"^[0-9]{1,2} %B [0-9]{2,4}[^0-9]\", date)\n",
    "    if not temp is None: return date_convert(temp.group()[:-1])\n",
    "    \n",
    "    # same as above but at anywhere in the string\n",
    "    temp = re.search(\"[^0-9][0-9]{1,2} %B [0-9]{2,4}[^0-9]\", date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:-1])\n",
    "\n",
    "    # same as above but it's the entire string\n",
    "    temp = re.search(\"^[0-9]{1,2} %B [0-9]{2,4}$\", date)\n",
    "    if not temp is None: return date_convert(temp.group())\n",
    "\n",
    "    # same as above but it's at the end of the string\n",
    "    temp = re.search(\"[^0-9][0-9]{1,2} %B [0-9]{2,4}$\", date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:])\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    # \"^Month [0|00], [00|000|0000]\"\n",
    "    temp = re.search(\"^%B [0-9]{1,2},? [0-9]{2,4}[^0-9]\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[:-1])\n",
    "    \n",
    "    # same as above but at anywhere in the string\n",
    "    temp = re.search(\"[^A-Z]%B [0-9]{1,2},? [0-9]{2,4}[^0-9]\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:-1])\n",
    "\n",
    "    # same as above but it's the entire string\n",
    "    temp = re.search(\"^[A-Z]%B [0-9]{1,2},? [0-9]{2,4}$\",date)\n",
    "    if not temp is None: return date_convert(temp.group())\n",
    "\n",
    "    # same as above but it's at the end of the string\n",
    "    temp = re.search(\"[^A-Z]%B [0-9]{1,2},? [0-9]{2,4}$\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:])\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    # \"^[00|000|0000]-00-[00|000|0000]\"\n",
    "    temp = re.search(\"^[0-9]{2,4}-[0-9]{2}-[0-9]{2,4}[^0-9]\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[:-1])\n",
    "\n",
    "    # same as above but at anywhere in the string\n",
    "    temp = re.search(\"[^0-9][0-9]{2,4}-[0-9]{2}-[0-9]{2,4}[^0-9]\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:-1])\n",
    "\n",
    "    # same as above but it's the entire string\n",
    "    temp = re.search(\"^[0-9]{2,4}-[0-9]{2}-[0-9]{2,4}$\",date)\n",
    "    if not temp is None: return date_convert(temp.group())\n",
    "\n",
    "    # same as above but it's at the end of the string\n",
    "    temp = re.search(\"[^0-9][0-9]{2,4}-[0-9]{2}-[0-9]{2,4}$\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:])\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    # \"^[00|000|0000]\"\n",
    "    temp = re.search(\"^[0-9]{2,4}[^0-9]\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[:-1])\n",
    "    \n",
    "    # same as above but at anywhere in the string\n",
    "    temp = re.search(\"[^0-9][0-9]{2,4}[^0-9]\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:-1])\n",
    "\n",
    "    # same as above but it's the entire string\n",
    "    temp = re.search(\"^[0-9]{2,4}$\",date)\n",
    "    if not temp is None: return date_convert(temp.group())\n",
    "\n",
    "    # same as above but it's at the end of the string\n",
    "    temp = re.search(\"[^0-9][0-9]{2,4}$\",date)\n",
    "    if not temp is None: return date_convert(temp.group()[1:])\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "#for i in range(len(phil_4)):\n",
    "#    p = list(phil_4.values())[i]\n",
    "#    print(\"index: \", i, \"\\t\",p[3],\"\\t\", p[4],\"\\n\")\n",
    "#    print(date_clean(p[3]),\"\\n\")\n",
    "#    print(date_clean(p[4]),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Born', datetime.date(716, 1, 1)], ['Died', datetime.date(778, 1, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "\n",
    "vars = [\"Born\",\"Died\"]\n",
    "\n",
    "# This function takes the prefix and ref of a philosopher's\n",
    "# wiki url as well as a list of variables (vars) that it scrapes\n",
    "# from the wiki page's info table. It does this by first identifying\n",
    "# the table (\"infobox biography vcard\" or \"tbody\") then finds the\n",
    "# rows whose title matches one of the variables that were passed. \n",
    "# It then takes the full html object in the content of that row,\n",
    "# wraps it into a list with the title ([title, content]) and appends\n",
    "# the list to a list of these pairs and returns that.\n",
    "def table_scrape(prefix, ref, vars):\n",
    "\n",
    "    page = requests.get(prefix + ref)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "    table = soup.find(\"table\", class_ = \"infobox biography vcard\")\n",
    "    if table is None: table = soup.find(\"tbody\")\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    row_dict = {}\n",
    "    for r in rows:\n",
    "        th = r.find(\"th\")\n",
    "        td = r.find(\"td\")\n",
    "        if not th is None and not td is None and th.text in vars:\n",
    "            if th.text in [\"Born\",\"Died\"]: row_dict[th.text] = date_clean(td.text)\n",
    "    \n",
    "    var_list = []\n",
    "    for var in vars:\n",
    "        if var in row_dict:\n",
    "            var_list.append([var,row_dict[var]])\n",
    "        else: var_list.append([var,None])\n",
    "        \n",
    "    return var_list\n",
    "\n",
    "table_scrape(prefix, \"/wiki/Sufyan_al-Thawri\", vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the correct wiki url prefix, \n",
    "# a dictionary of philosophers as produced by\n",
    "# iterated_crawl, and a list of variables to scrape.\n",
    "# It iterates through the dictionary of philosophers\n",
    "# and adds the variables produced by table_scrape for\n",
    "# each philosopher.\n",
    "def add_info(prefix, phils, vars):\n",
    "\n",
    "    for key, value in phils.items():\n",
    "        attrs = table_scrape(prefix,value[0],vars)\n",
    "        for a in attrs:\n",
    "            value.append(a[1])\n",
    "\n",
    "    return phils\n",
    "\n",
    "phil_4 = add_info(prefix, phil_3, vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function creates a list of all the directed edges by \n",
    "# taking the union of connections. (e.g. if it's listed\n",
    "# on Descartes page that he influenced Kant but it \n",
    "# doesn't list on Kant's page that he was influenced\n",
    "# by Descartes it will appear as a connection). It\n",
    "# takes in the network as collected by iterated_crawl\n",
    "# then returns a list of pairs where the first entry\n",
    "# in each pair influenced the second entry in the pair\n",
    "def edge_finder(network):\n",
    "\n",
    "    edges = []\n",
    "\n",
    "    for key, value in network.items():\n",
    "\n",
    "        for infs in value[1]:\n",
    "            if infs in network:\n",
    "                edges.append([infs,key])\n",
    "\n",
    "        for infd in value[2]:\n",
    "            if infd in network:\n",
    "                temp = [key,infd]\n",
    "                if not temp in edges:\n",
    "                    edges.append(temp)\n",
    "\n",
    "    return edges\n",
    "\n",
    "edges = edge_finder(phil_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts the network and list of edges into a standard\n",
    "# format .gdf file which is typically how network data is stored.\n",
    "# For more relevant information on this file format look here:\n",
    "# https://gephi.org/users/supported-graph-formats/gdf-format/\n",
    "with open(\"philosophers.gdf\", \"w\", encoding = \"utf-8\") as f:\n",
    "    with open(\"nodes.csv\", \"w\", encoding = \"utf-8\") as n:\n",
    "    \n",
    "        fwrite = csv.writer(f, lineterminator = '\\n') \n",
    "        nwrite = csv.writer(n, lineterminator = '\\n') \n",
    "    \n",
    "        node_list = {}\n",
    "\n",
    "        fwrite.writerow([\"nodedef>name VARCHAR\",\"label VARCHAR\",\"born VARCHAR\",\"died VARCHAR\"])\n",
    "        nwrite.writerow([\"id\",\"label\",\"born\",\"died\"])\n",
    "\n",
    "        ctr = 0\n",
    "        for key, value in phil_4.items():\n",
    "            ctr += 1\n",
    "            node_list[key] = \"n\" + str(ctr)\n",
    "            fwrite.writerow([\"n\"+ str(ctr),key,value[3],value[4]])\n",
    "            nwrite.writerow([\"n\"+ str(ctr),key,value[3],value[4]])\n",
    "            #f.write(\"\\n\")\n",
    "\n",
    "    with open(\"edges.csv\", \"w\", encoding = \"utf-8\") as e:\n",
    "\n",
    "        ewrite = csv.writer(e, lineterminator = '\\n') \n",
    "\n",
    "        fwrite.writerow([\"edgedef>node1 VARCHAR\",\"node2 VARCHAR\",\"directed BOOLEAN\"])\n",
    "        ewrite.writerow([\"from\",\"to\"])\n",
    "\n",
    "        for edge in edges:\n",
    "            fwrite.writerow([node_list[edge[0]], node_list[edge[1]],\"true\"])\n",
    "            ewrite.writerow([node_list[edge[0]], node_list[edge[1]]])\n"
   ]
  }
 ]
}